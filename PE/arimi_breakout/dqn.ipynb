{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "\n",
    "# ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "ENV_NAME = 'ALE/Breakout-v5'\n",
    "# ENV_NAME = 'PongDeterministic-v4'  \n",
    "# You can increase the learning rate to 0.00025 in Pong for quicker results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of DeepMind's Deep Q-Learning by Fabio M. Graetz, 2018\n",
    "If you have questions or suggestions, write me a mail fabiograetzatgooglemaildotcom\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import imageio\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=<unknown> dtype=uint8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "tf.compat.v1.placeholder(\n",
    "    tf.uint8, shape=None, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor(object):\n",
    "    \"\"\"Resizes and converts RGB Atari frames to grayscale\"\"\"\n",
    "    def __init__(self, frame_height=84, frame_width=84):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "        \"\"\"\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.frame = tf.compat.v1.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize(self.processed, \n",
    "                                                [self.frame_height, self.frame_width], \n",
    "                                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def __call__(self, session, frame):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            session: A Tensorflow session object\n",
    "            frame: A (210, 160, 3) frame of an Atari game in RGB\n",
    "        Returns:\n",
    "            A processed (84, 84, 1) frame in grayscale\n",
    "        \"\"\"\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    \"\"\"Implements a Deep Q Network\"\"\"\n",
    "    \n",
    "    # pylint: disable=too-many-instance-attributes\n",
    "    \n",
    "    def __init__(self, n_actions, hidden=1024, learning_rate=0.00001, \n",
    "                 frame_height=84, frame_width=84, agent_history_length=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_actions: Integer, number of possible actions\n",
    "            hidden: Integer, Number of filters in the final convolutional layer. \n",
    "                    This is different from the DeepMind implementation\n",
    "            learning_rate: Float, Learning rate for the Adam optimizer\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.hidden = hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        \n",
    "        self.input = tf.compat.v1.placeholder(shape=[None, self.frame_height, \n",
    "                                           self.frame_width, self.agent_history_length], \n",
    "                                    dtype=tf.float32)\n",
    "        # Normalizing the input\n",
    "        self.inputscaled = self.input/255\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            batch_size=self.inputscaled, filters=32, kernel_size=[8, 8], strides=4,\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv1')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            batch_size=self.conv1, filters=64, kernel_size=[4, 4], strides=2, \n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv2')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            batch_size=self.conv2, filters=64, kernel_size=[3, 3], strides=1, \n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv3')\n",
    "        self.conv4 = tf.keras.layers.Conv2D(\n",
    "            batch_size=self.conv3, filters=hidden, kernel_size=[7, 7], strides=1, \n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2),\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv4')\n",
    "        \n",
    "        # Splitting into value and advantage stream\n",
    "        self.valuestream, self.advantagestream = tf.split(self.conv4, 2, 3)\n",
    "        self.valuestream = tf.keras.layers.flatten(self.valuestream)\n",
    "        self.advantagestream = tf.keras.layers.flatten(self.advantagestream)\n",
    "        self.advantage = tf.keras.layers.dense(\n",
    "            batch_size=self.advantagestream, units=self.n_actions,\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2), name=\"advantage\")\n",
    "        self.value = tf.keras.layers.dense(\n",
    "            batch_size=self.valuestream, units=1, \n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2), name='value')\n",
    "        \n",
    "        # Combining value and advantage into Q-values as described above\n",
    "        self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        # The next lines perform the parameter update. This will be explained in detail later.\n",
    "        \n",
    "        # targetQ according to Bellman equation: \n",
    "        # Q = r + gamma*max Q', calculated in the function learn()\n",
    "        self.target_q = tf.compat.v1.placeholder(shape=[None], dtype=tf.float32)\n",
    "        # Action that was performed\n",
    "        self.action = tf.compat.v1.placeholder(shape=[None], dtype=tf.int32)\n",
    "        # Q value of the action that was performed\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.n_actions, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        # Parameter updates\n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions=self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplorationExploitationScheduler(object):\n",
    "    \"\"\"Determines an action according to an epsilon greedy strategy with annealing epsilon\"\"\"\n",
    "    def __init__(self, DQN, n_actions, eps_initial=1, eps_final=0.1, eps_final_frame=0.01, \n",
    "                 eps_evaluation=0.0, eps_annealing_frames=1000000, \n",
    "                 replay_memory_start_size=50000, max_frames=25000000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            DQN: A DQN object\n",
    "            n_actions: Integer, number of possible actions\n",
    "            eps_initial: Float, Exploration probability for the first \n",
    "                replay_memory_start_size frames\n",
    "            eps_final: Float, Exploration probability after \n",
    "                replay_memory_start_size + eps_annealing_frames frames\n",
    "            eps_final_frame: Float, Exploration probability after max_frames frames\n",
    "            eps_evaluation: Float, Exploration probability during evaluation\n",
    "            eps_annealing_frames: Int, Number of frames over which the \n",
    "                exploration probabilty is annealed from eps_initial to eps_final\n",
    "            replay_memory_start_size: Integer, Number of frames during \n",
    "                which the agent only explores\n",
    "            max_frames: Integer, Total number of frames shown to the agent\n",
    "        \"\"\"\n",
    "        self.n_actions = n_actions\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "        self.replay_memory_start_size = replay_memory_start_size\n",
    "        self.max_frames = max_frames\n",
    "        \n",
    "        # Slopes and intercepts for exploration decrease\n",
    "        self.slope = -(self.eps_initial - self.eps_final)/self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_memory_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame)/(self.max_frames - self.eps_annealing_frames - self.replay_memory_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "        \n",
    "        self.DQN = DQN\n",
    "\n",
    "    def get_action(self, session, frame_number, state, evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            session: A tensorflow session object\n",
    "            frame_number: Integer, number of the current frame\n",
    "            state: A (84, 84, 4) sequence of frames of an Atari game in grayscale\n",
    "            evaluation: A boolean saying whether the agent is being evaluated\n",
    "        Returns:\n",
    "            An integer between 0 and n_actions - 1 determining the action the agent perfoms next\n",
    "        \"\"\"\n",
    "        if evaluation:\n",
    "            eps = self.eps_evaluation\n",
    "        elif frame_number < self.replay_memory_start_size:\n",
    "            eps = self.eps_initial\n",
    "        elif frame_number >= self.replay_memory_start_size and frame_number < self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope*frame_number + self.intercept\n",
    "        elif frame_number >= self.replay_memory_start_size + self.eps_annealing_frames:\n",
    "            eps = self.slope_2*frame_number + self.intercept_2\n",
    "        \n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "        return session.run(self.DQN.best_action, feed_dict={self.DQN.input:[state]})[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:53: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:53: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_2234/1792354054.py:53: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if self.count is 0:\n"
     ]
    }
   ],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Replay Memory that stores the last size=1,000,000 transitions\"\"\"\n",
    "    def __init__(self, size=1000000, frame_height=84, frame_width=84, \n",
    "                 agent_history_length=4, batch_size=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size: Integer, Number of stored transitions\n",
    "            frame_height: Integer, Height of a frame of an Atari game\n",
    "            frame_width: Integer, Width of a frame of an Atari game\n",
    "            agent_history_length: Integer, Number of frames stacked together to create a state\n",
    "            batch_size: Integer, Number if transitions returned in a minibatch\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        \n",
    "        # Pre-allocate memory for the states and new_states in a minibatch\n",
    "        self.states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                    self.frame_height, self.frame_width), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "    def add_experience(self, action, frame, reward, terminal):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: An integer between 0 and env.action_space.n - 1 \n",
    "                determining the action the agent perfomed\n",
    "            frame: A (84, 84, 1) frame of an Atari game in grayscale\n",
    "            reward: A float determining the reward the agend received for performing an action\n",
    "            terminal: A bool stating whether the episode terminated\n",
    "        \"\"\"\n",
    "        if frame.shape != (self.frame_height, self.frame_width):\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "             \n",
    "    def _get_state(self, index):\n",
    "        if self.count is 0:\n",
    "            raise ValueError(\"The replay memory is empty!\")\n",
    "        if index < self.agent_history_length - 1:\n",
    "            raise ValueError(\"Index must be min 3\")\n",
    "        return self.frames[index-self.agent_history_length+1:index+1, ...]\n",
    "        \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "            \n",
    "    def get_minibatch(self):\n",
    "        \"\"\"\n",
    "        Returns a minibatch of self.batch_size = 32 transitions\n",
    "        \"\"\"\n",
    "        if self.count < self.agent_history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "        \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "        \n",
    "        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(session, replay_memory, main_dqn, target_dqn, batch_size, gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        session: A tensorflow sesson object\n",
    "        replay_memory: A ReplayMemory object\n",
    "        main_dqn: A DQN object\n",
    "        target_dqn: A DQN object\n",
    "        batch_size: Integer, Batch size\n",
    "        gamma: Float, discount factor for the Bellman equation\n",
    "    Returns:\n",
    "        loss: The loss of the minibatch, for tensorboard\n",
    "    Draws a minibatch from the replay memory, calculates the \n",
    "    target Q-value that the prediction Q-value is regressed to. \n",
    "    Then a parameter update is performed on the main DQN.\n",
    "    \"\"\"\n",
    "    # Draw a minibatch from the replay memory\n",
    "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_minibatch()    \n",
    "    # The main network estimates which action is best (in the next \n",
    "    # state s', new_states is passed!) \n",
    "    # for every transition in the minibatch\n",
    "    arg_q_max = session.run(main_dqn.best_action, feed_dict={main_dqn.input:new_states})\n",
    "    # The target network estimates the Q-values (in the next state s', new_states is passed!) \n",
    "    # for every transition in the minibatch\n",
    "    q_vals = session.run(target_dqn.q_values, feed_dict={target_dqn.input:new_states})\n",
    "    double_q = q_vals[range(batch_size), arg_q_max]\n",
    "    # Bellman equation. Multiplication with (1-terminal_flags) makes sure that \n",
    "    # if the game is over, targetQ=rewards\n",
    "    target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
    "    # Gradient descend step to update the parameters of the main network\n",
    "    loss, _ = session.run([main_dqn.loss, main_dqn.update], \n",
    "                          feed_dict={main_dqn.input:states, \n",
    "                                     main_dqn.target_q:target_q, \n",
    "                                     main_dqn.action:actions})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetworkUpdater(object):\n",
    "    \"\"\"Copies the parameters of the main DQN to the target DQN\"\"\"\n",
    "    def __init__(self, main_dqn_vars, target_dqn_vars):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            main_dqn_vars: A list of tensorflow variables belonging to the main DQN network\n",
    "            target_dqn_vars: A list of tensorflow variables belonging to the target DQN network\n",
    "        \"\"\"\n",
    "        self.main_dqn_vars = main_dqn_vars\n",
    "        self.target_dqn_vars = target_dqn_vars\n",
    "\n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_dqn_vars):\n",
    "            copy_op = self.target_dqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "            \n",
    "    def __call__(self, sess):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "        Assigns the values of the parameters of the main network to the \n",
    "        parameters of the target network\n",
    "        \"\"\"\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(frame_number, frames_for_gif, reward, path):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            frame_number: Integer, determining the number of the current frame\n",
    "            frames_for_gif: A sequence of (210, 160, 3) frames of an Atari game in RGB\n",
    "            reward: Integer, Total reward of the episode that es ouputted as a gif\n",
    "            path: String, path where gif is saved\n",
    "    \"\"\"\n",
    "    for idx, frame_idx in enumerate(frames_for_gif): \n",
    "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
    "                                     preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{path}{\"ATARI_frame_{0}_reward_{1}.gif\".format(frame_number, reward)}', \n",
    "                    frames_for_gif, duration=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "tf.compat.v1.placeholder(\n",
    "    tf.uint8, shape=None, name=None\n",
    ")\n",
    "\n",
    "class Atari(object):\n",
    "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
    "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
    "        self.env = gym.make(envName)\n",
    "        self.process_frame = FrameProcessor()\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.agent_history_length = agent_history_length\n",
    "\n",
    "    def reset(self, sess, evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            evaluation: A boolean saying whether the agent is evaluating or training\n",
    "        Resets the environment and stacks four frames ontop of each other to \n",
    "        create the first state\n",
    "        \"\"\"\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True # Set to true so that the agent starts \n",
    "                                  # with a 'FIRE' action when evaluating\n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
    "        processed_frame = self.process_frame(sess, frame)   # (★★★)\n",
    "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "\n",
    "    def step(self, sess, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            action: Integer, action the agent performs\n",
    "        Performs an action and observes the reward and terminal state from the environment\n",
    "        \"\"\"\n",
    "        new_frame, reward, terminal, info = self.env.step(action)  # (5★)\n",
    "            \n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "        \n",
    "        processed_new_frame = self.process_frame(sess, new_frame)   # (6★)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_reward(reward):\n",
    "    if reward > 0:\n",
    "        return 1\n",
    "    elif reward == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "tf.summary.create_file_writer('log_path')\n",
    "\n",
    "\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Control parameters\n",
    "MAX_EPISODE_LENGTH = 18000       # Equivalent of 5 minutes of gameplay at 60 frames per second\n",
    "EVAL_FREQUENCY = 200000          # Number of frames the agent sees between evaluations\n",
    "EVAL_STEPS = 10000               # Number of frames for one evaluation\n",
    "NETW_UPDATE_FREQ = 10000         # Number of chosen actions between updating the target network. \n",
    "                                 # According to Mnih et al. 2015 this is measured in the number of \n",
    "                                 # parameter updates (every four actions), however, in the \n",
    "                                 # DeepMind code, it is clearly measured in the number\n",
    "                                 # of actions the agent choses\n",
    "DISCOUNT_FACTOR = 0.99           # gamma in the Bellman equation\n",
    "REPLAY_MEMORY_START_SIZE = 50000 # Number of completely random actions, \n",
    "                                 # before the agent starts learning\n",
    "MAX_FRAMES = 30000000            # Total number of frames the agent sees \n",
    "MEMORY_SIZE = 1000000            # Number of transitions stored in the replay memory\n",
    "NO_OP_STEPS = 10                 # Number of 'NOOP' or 'FIRE' actions at the beginning of an \n",
    "                                 # evaluation episode\n",
    "UPDATE_FREQ = 4                  # Every four actions a gradient descend step is performed\n",
    "HIDDEN = 1024                    # Number of filters in the final convolutional layer. The output \n",
    "                                 # has the shape (1,1,1024) which is split into two streams. Both \n",
    "                                 # the advantage stream and value stream have the shape \n",
    "                                 # (1,1,512). This is slightly different from the original \n",
    "                                 # implementation but tests I did with the environment Pong \n",
    "                                 # have shown that this way the score increases more quickly\n",
    "LEARNING_RATE = 0.00001          # Set to 0.00025 in Pong for quicker results. \n",
    "                                 # Hessel et al. 2017 used 0.0000625\n",
    "BS = 32                          # Batch size\n",
    "\n",
    "PATH = \"output/\"                 # Gifs and checkpoints will be saved here\n",
    "SUMMARIES = \"summaries\"          # logdir for tensorboard\n",
    "RUNID = 'run_1'\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(SUMMARIES, RUNID), exist_ok=True)\n",
    "SUMM_WRITER = tf.summary.create_file_writer(os.path.join(SUMMARIES, RUNID))\n",
    "\n",
    "atari = Atari(ENV_NAME, NO_OP_STEPS)\n",
    "\n",
    "print(\"The environment has the following {} actions: {}\".format(atari.env.action_space.n, \n",
    "                                                                atari.env.unwrapped.get_action_meanings()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got an unexpected keyword argument 'kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# main DQN and target DQN networks:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mvariable_scope(\u001b[39m'\u001b[39m\u001b[39mmainDQN\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     MAIN_DQN \u001b[39m=\u001b[39m DQN(atari\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn, HIDDEN, LEARNING_RATE)   \u001b[39m# (★★)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mvariable_scope(\u001b[39m'\u001b[39m\u001b[39mtargetDQN\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      5\u001b[0m     TARGET_DQN \u001b[39m=\u001b[39m DQN(atari\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn, HIDDEN)               \u001b[39m# (★★)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [33], line 32\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[0;34m(self, n_actions, hidden, learning_rate, frame_height, frame_width, agent_history_length)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputscaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[39m# Convolutional layers\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mconv2d(\n\u001b[1;32m     33\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minputscaled, filters\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m[\u001b[39m8\u001b[39;49m, \u001b[39m8\u001b[39;49m], strides\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,\n\u001b[1;32m     34\u001b[0m     kernel_initializer\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49minitializers\u001b[39m.\u001b[39;49mVarianceScaling(scale\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m),\n\u001b[1;32m     35\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m\"\u001b[39;49m, activation\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mrelu, use_bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mconv1\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mconv2d(\n\u001b[1;32m     37\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1, filters\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m[\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m], strides\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \n\u001b[1;32m     38\u001b[0m     kernel_initializer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39minitializers\u001b[39m.\u001b[39mVarianceScaling(scale\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[1;32m     39\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m, activation\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu, use_bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconv2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mconv2d(\n\u001b[1;32m     41\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2, filters\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m[\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m], strides\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[1;32m     42\u001b[0m     kernel_initializer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39minitializers\u001b[39m.\u001b[39mVarianceScaling(scale\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[1;32m     43\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m, activation\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu, use_bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconv3\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1170\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[39mif\u001b[39;00m iterable_params \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m   args, kwargs \u001b[39m=\u001b[39m replace_iterable_params(args, kwargs, iterable_params)\n\u001b[0;32m-> 1170\u001b[0m result \u001b[39m=\u001b[39m api_dispatcher\u001b[39m.\u001b[39;49mDispatch(args, kwargs)\n\u001b[1;32m   1171\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1172\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: Got an unexpected keyword argument 'kernel_size'"
     ]
    }
   ],
   "source": [
    "# main DQN and target DQN networks:\n",
    "with tf.compat.v1.variable_scope('mainDQN'):\n",
    "    MAIN_DQN = DQN(atari.env.action_space.n, HIDDEN, LEARNING_RATE)   # (★★)\n",
    "with tf.compat.v1.variable_scope('targetDQN'):\n",
    "    TARGET_DQN = DQN(atari.env.action_space.n, HIDDEN)               # (★★)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()    \n",
    "\n",
    "MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n",
    "TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9819c38625e3d453af23400f3105fdafaf1e618a5986311776fa7a8a0649173a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
